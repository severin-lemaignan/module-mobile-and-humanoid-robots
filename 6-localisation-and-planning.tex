%!TEX program = xelatex

\documentclass[compress]{beamer}
%--------------------------------------------------------------------------
% Common packages
%--------------------------------------------------------------------------

\definecolor{links}{HTML}{663000}
\hypersetup{colorlinks,linkcolor=,urlcolor=links}

\usepackage[english]{babel}
\usepackage{pgfpages} % required for notes on second screen
\usepackage{graphicx}

\usepackage{multicol}

\usepackage{tabularx,ragged2e}
\usepackage{booktabs}

\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}


\usetheme{hri}

% Display the navigation bullet even without subsections
\usepackage{remreset}% tiny package containing just the \@removefromreset command
\makeatletter
\@removefromreset{subsection}{section}
\makeatother
\setcounter{subsection}{1}


\newcommand{\source}[2]{{\tiny\it Source: \href{#1}{#2}}}

\newcommand{\colvec}[2][.8]{%
  \scalebox{#1}{%
    \renewcommand{\arraystretch}{.8}%
    $\begin{bmatrix}#2\end{bmatrix}$%
  }
}


\usepackage{tikz}
\usetikzlibrary{mindmap,backgrounds,positioning,quotes,shapes}

\graphicspath{{figs/part6/}}

\title{ROCO318 \newline Mobile and Humanoid Robots}
\subtitle{Part 6 -- Localisation and Planning}

\date{}
\author{Séverin Lemaignan}
\institute{Centre for Neural Systems and Robotics\\{\bf Plymouth University}}

\begin{document}

\licenseframe{github.com/severin-lemaignan/module-mobile-and-humanoid-robots}

\maketitle

\begin{frame}{Pose and navigation}

Pose maintenance

\begin{itemize}
\item \textbf{Pose} (= position and orientation) \textbf{maintenance} is
  keeping a precise as possible estimate of the robot's pose.
\end{itemize}

Getting from A to B.

\begin{itemize}
\item Different from inverse kinematics, where an efficient manner is
  computed to get a robot in a certain state (position and orientation).
  Inverse kinematics work on a small scale (\textless{} meter).
\item Planning and navigation work on a larger scale (room, building,
  roadmap, \ldots{}).
\end{itemize}

\end{frame}

\begin{frame}{Planning and navigation -- traditional approach}

Today's industrial robots can operate \textbf{without any intelligence} because
their environment is static and very structured.

In mobile robotics, cognition and reasoning is primarily of geometric
nature, such as picking a safe path or determining where to go next.

\begin{itemize}
\item already been largely explored in literature for cases in which
  complete information about the current situation and the environment
  exists (e.g.
  \href{http://en.wikipedia.org/wiki/Travelling_salesman_problem}{travelling
  sales man problem} or
  \href{http://en.wikipedia.org/wiki/Route_inspection_problem}{route
  inspection problem}).
\end{itemize}

\end{frame}

\begin{frame}{Kiva systems: Beacon based localisation}
    \begin{center}
        \url{http://www.youtube.com/watch?v=lWsMdN7HMuA}

        \includegraphics[width=0.8\linewidth]{kiva}
    \end{center}
\end{frame}

\section{Monte-Carlo Localisation}

\begin{frame}{Monte Carlo methods}

    \begin{columns}
        \begin{column}{0.5\linewidth}
            \textbf{Repeated random sampling to compute results}

            Used when it to difficult to calculate exact results:

            \begin{itemize}
                \item The system is too complex and calculating it through is too
                    computationally expensive
                \item There are too many variables and it is impossible to calculate all
                    posibilities
            \end{itemize}


        \end{column}
        \begin{column}{0.5\linewidth}
            \begin{center}
                \includegraphics[width=\linewidth]{montecarlo}
            \end{center}

            \onslide<2>{
            Monte Carlo \emph{localisation} is MC methods applied to robot localisation.
            }
        \end{column}
    \end{columns}

\end{frame}

\videoframe[0.56]{figs/part6/mcl_example.mp4}

\imageframe[color=black]{localisation_pepper}

\begin{frame}{Monte-Carlo Localisation -- Introduction}

    \begin{itemize}
        \item Monte-Carlo Localisation (MCL) is a \href{http://en.wikipedia.org/wiki/Particle_filter}{particle
            filter}, which uses a set $S_t$ of $N$ particles at time
            $t$.
            \large
            \begin{align*}
                S_t &= \{s_t^i \quad|\quad i = 1\ldots N\} \\
                s^i &= \{\mathbf{x}^i, w^i\} \text{ with } \mathbf{x}^i = \{x^i, y^i, \theta^i\}
            \end{align*}
            \normalsize
        \item<+-> For small maps, $N$ is in the hundreds, for larger maps you
            can have thousands of particles.
        \item<+-> Each particle $s^i$ contains the pose of the particle and a weight $w^i$.
        \item<+-> The sum of all weights is 1: \[ \sum_{i=1}^N w^i = 1 \]
    \end{itemize}

\end{frame}

\begin{frame}{Monte-Carlo Localisation -- Initialisation}

    The $N$ particles are distributed on the map

    \begin{itemize}
        \item If the robot's position is not known, they are distributed randomly
            across the map with a probability $p=\frac{1}{N}$ (\emph{uniform
            distribution}). \textbf{Global localisation}.
            \pause
        \item If the robot's position is available, the initial particle
            distribution could be Gaussian around the robot (\emph{normal
            distribution}). \textbf{Continuous localisation} $\equiv$
            \textbf{Tracking}.
    \end{itemize}

    \pause 

    Then , Monte Carlo Localisation runs through three steps:

    \begin{enumerate}
        \item Prediction phase (when the robot takes an action)
        \item Update phase (when sensor data comes in)
        \item Resampling phase
    \end{enumerate}

\end{frame}

\begin{frame}{Monte-Carlo Localisation -- Prediction phase}

The robot performs an \textbf{action.} All $N$ particles are
    updated as follows:
    \Large
        \[ 
        \bubblemark{newp}p(s_t^i) = \sum_{j=1}^N
        p(l_t{}^j\bubblemark[0pt]{motionmodel}|l_{t-1}{}^i, a) \cdot p(s\bubblemark[0pt]{oldp}_{t-1}{}^i)
        \]
    \normalsize
    \begin{columns}
        \begin{column}{0.5\linewidth}
Example: repeated application of the prediction rule leads to gradual
  loss of pose accuracy.
        \end{column}
        \begin{column}{0.5\linewidth}
            \begin{center}
                \includegraphics[width=\linewidth]{mcl_prediction}
            \end{center}
        \end{column}
    \end{columns}

    \bubble<1>[40][1][6.5cm]{motionmodel}{Motion model: what is the probability of ending up in the location of
particle $s_t$ after doing action $a$ at the location of
    particle $s_{t-1}$}

    \bubble<1>[130][0.5][3cm]{oldp}{Belief of $i^{th}$ particle at time $t-1$}

    \bubble<1>[0]{newp}{Updated belief of particle $i$}

\end{frame}

\begin{frame}{Monte-Carlo Loclaisation - Prediction phase}

The prediction phase \textbf{essentially moves all particles}.

Example: if the robot moves 10cm right and turn 10˚ counterclockwise, then all particles are
updated:
    \Large
    \[
        s_t^i = \{ \{ x_{t-1}^i + 0.1, y_{t-1}^i, \theta_{t-1}^i - 10^{\circ}\}, w\bubblemark[0pt]{oldnochange}_{t-1}^i\}
    \]
    \normalsize

\bubble<1>[90]{oldnochange}{Note, the weight is not changed}
\pause

    This allows for a motion model to be implemented, by adding some noise.

    \only<2>{
    \large
    \begin{align*}
        s_t^1 &= \{ \{ x_{t-1}^1 + 0.11, y_{t-1}^1, \theta_{t-1}^1 - 10.3^{\circ}\}, w_{t-1}^1\} \\
        s_t^2 &= \{ \{ x_{t-1}^2 + 0.09, y_{t-1}^2, \theta_{t-1}^2 - 10.2^{\circ}\}, w_{t-1}^2\} \\
        ...
    \end{align*}
    \normalsize
    }

    \only<3>{
    This motion model should be based on the kinematics of your robot.
    \eg for a differential drive robot you would have a pretty accurate
    idea of how imprecise it is.
    }


\end{frame}

\begin{frame}{Monte-Carlo Localisation -- Update phase}

    \begin{itemize}
        \item The robot gets \textbf{new sensor data} in, all $N$ particles
            updated as follows:
            \Large
            \[ 
            w_t^i = \bubblemark[0pt]{alpha}\alpha \cdot p(z_t\bubblemark[0pt]{sensormodel}|\mathbf{x}_t^i) \cdot w_t^i
            \]
            \normalsize
            \vspace{3em}
        \item The normalising constant $\alpha$ is calculated afterwards to make
            \[ \sum_{i=1}^N w^i = 1 \]
    \end{itemize}

    \bubble<1>[145][.8][4cm]{sensormodel}{Sensor model: what is the probability
    of seeing sensor reading $z_t$ at position $\mathbf{x}_t$}

    \bubble<1>[40][.8][3cm]{alpha}{Normalisation constant: keeps the sum of $w^i$ equal
    to 1}

\end{frame}

\begin{frame}{Monte-Carlo Localisation -- Resampling phase}

    Generate a new set of particles by drawing randomly from the current
    set. Likelihood is determined by the weights values.

    %% Really??? Reset the weights??
    %Then, reset the weights $w$ of these new samples to $\frac{1}{N}$.

    \begin{overprint}
        \onslide<1>
        \vspace{1em}
        \small
        You can think of drawing particles as a roulette wheel selection.
        Particles with a higher weight have a higher chance of being
        selected into the new set. Also, a particle can be selected \textbf{more than
        once}.

        \centering
        \resizebox{0.4\linewidth}{!}{
            \begin{tikzpicture}[>=latex]

                \filldraw[fill=hriSec3Comp,ultra thick] (0,0) circle (3cm);
                \fill (0,0) circle (0.3cm);
                \draw[thick] (0,0) -- (10:3cm);
                \draw[thick] (0,0) -- (40:3cm);
                \draw[thick] (0,0) -- (60:3cm);
                \draw[thick] (0,0) -- (110:3cm);
                \draw[thick] (0,0) -- (150:3cm);
                \draw[thick] (0,0) -- (180:3cm);
                \draw[thick] (0,0) -- (230:3cm);
                \draw[thick] (0,0) -- (300:3cm);
                \draw[thick] (0,0) -- (330:3cm);

                \node at (25:2cm) {$w^1$};
                \node at (50:2cm) {$w^2$};
                \node at (85:2cm) {$w^3$};
                \node at (130:2cm) {$w^4$};
                \node at (165:2cm) {$w^5$};
                \node at (205:2cm) {$w^6$};
                \node at (275:2cm) {$w^7$};
                \node at (315:2cm) {$w^8$};
                \node at (350:2cm) {$w^9$};

                \node[signal,draw,fill=hriSec3Dark,minimum size=1cm,signal to=west] at (3.5,0) {};

            \end{tikzpicture}
        }

        \onslide<2>
        \vspace{2em}
        In addition, it is useful to add a \textbf{small number of uniformly distributed
        particles} to the set of particles.

        This helps the algorithm recover when it loses track of the robot's
        position.

        \vspace{1em}
        \small
        This is know as the \emph{``kidnapped robot''} problem, and occurs when the
        robot loses track of its position, due to a malfunction or power
        outage.


    \end{overprint}

\end{frame}

\begin{frame}[fragile]{Pseudocode}

    \begin{center}

\begin{overprint}
    \onslide<1>
\begin{pythoncode*}{frame=none,escapeinside=||}

# initialise all N particles
for i in range(1, N):
    S[i].x = random(x_max)
    S[i].y = random(y_max)
    S[i].|$\theta$| = random(2 * |$\pi$|)
    S[i].w = 1/N
\end{pythoncode*}

\onslide<2>
\begin{pythoncode*}{frame=none,escapeinside=\%\%}
while True:
    # prediction phase
    move_robot(x, y, %$\theta$%)
    for i in range(1,N):
        move_particle(i, x, y, %$\theta$%) # adds noise as well -> motion model

    # update phase (r: sensor reading)
    n = 0
    for i in range(1,N):
        S[i].w = likelihood(r, (S[i].x, S[i].y, S[i].%$\theta$%)) * S[i].w
        n = n + S[i].w

    # normalise
    for i in range(1,N):
        S[i].w = S[i].w/n

    # resample phase
    S1 = best_particles() # particles with high S.w
    S2 = worst_particles() # particles with low S.w
    replace_particles(S2, S1)
\end{pythoncode*}

\end{overprint}

    \end{center}
\end{frame}

\begin{frame}{Position estimate}

    The position estimate at time $t$ is the mean of the weighted particles locations,
    \ie the centre of the weighted particle cloud.

    \Large
    \[
        \bar{\mathbf{x}}_t = \sum^N_{i=1} w^i_t \cdot \mathbf{x}^i_t
    \]
\end{frame}

\begin{frame}{MCL -- Some properties}

Light on memory and computational resources.

\begin{itemize}
\item Markov Localisation uses much more memory, as it needs to keep track
  of belief at each location of the map even the ones that have a low
  belief. This also results in a computationally expensive algorithm.
\end{itemize}

``Any time'' algorithm: you can interrupt the algorithm and still get an
estimate out.

Powerful, yet efficient.

Easy to implement.

Compared to Kalman filtering

    \begin{itemize}
        \item Kalman filtering doe snot work for multi-modal distributions of
            the position estimate (\emph{data association problem})
    \end{itemize}
\end{frame}

\begin{frame}{Example: museum pose maintenance}

Monte Carlo using the ceiling of the Smithsonian museum

    \begin{center}
    \video{0.6\linewidth}{figs/part6/mcl.mp4}
    \end{center}

\end{frame}

\begin{frame}{Example: AIBO localising}

Estimating the robots position using Rao-Blackwellised particle filters

\begin{itemize}
\item D. Fox, http://www.cs.washington.edu/ai/Mobile\_Robotics/Aibo
\end{itemize}

\end{frame}

\begin{frame}{MCL -- Further reading}

Frank Dellaert,~Dieter Fox, Wolfram Burgard, and~Sebastian Thrun,
"\href{http://www.ri.cmu.edu/publication_view.html?pub_id=533}{Monte
Carlo Localization for Mobile Robots}"~\emph{IEEE International
Conference on Robotics and Automation (ICRA99)}, May, 1999.

Videos of examples of MCL

\begin{itemize}
\item \url{http://www.youtube.com/watch?v=uU_1c_CxB1g}
\item \url{http://www.youtube.com/watch?v=7K8dZwqBSSA}
\item \url{https://www.youtube.com/watch?v=oKUYj1FWzN4}
\item \url{https://www.youtube.com/watch?v=lCXv4yOcwf8}
\end{itemize}

\end{frame}

\section{SLAM: simultaneous localisation and mapping}

\begin{frame}{SLAM simultaneous localisation and mapping}

Content for these slides is taken from
\href{http://www.computerrobotvision.org/2010/slam_camp.html}{Jack
Collier}

\end{frame}

\begin{frame}{What is SLAM?}

Given an unknown environment and vehicle pose:

\begin{itemize}
\item Move through the environment
\item Estimate the robot pose
\item Generate a map of environmental features
\end{itemize}

Use the robot pose estimate to improve the map landmark position
estimates

Use the landmark estimates to improve the robot pose estimate

\end{frame}

\begin{frame}{Why is SLAM hard?}

Chicken and egg problem

\begin{itemize}
\item If there is no map, how the robot localise itself. If the robot
  doesn't know its location, how can it build up a map?
\end{itemize}

Odometry is not to be trusted

    \begin{center}
        \includegraphics[width=0.4\linewidth]{slam1}
        \hspace{1em}
        \includegraphics[width=0.4\linewidth]{slam2}
    \end{center}
\end{frame}

\begin{frame}[plain]

    \begin{center}
    \video[1.0]{0.7\linewidth}{figs/part6/slam.mp4}

    \source{http://youtu.be/Q1ipn42rMh8}{Robert Sim, University of British Columbia}
    \end{center}

\end{frame}

\imageframe{mapping_pepper}

\begin{frame}{Commercial applications}

\begin{itemize}
\item \href{https://www.youtube.com/watch?v=bq5HZzGF3vQ}{Samsung vacuum
  cleaning robot}
\item \href{https://www.youtube.com/watch?v=bq5HZzGF3vQ}{iRobot having SLAM}
\end{itemize}

\end{frame}

\section{Path planning}

\begin{frame}{Global vs local path planning}

        \begin{tikzpicture}[>=latex]
            \node at (0,0) {\includegraphics[width=\linewidth]{motion_planning_pepper_rviz}};

            \only<2>{
            \draw[thick,->] (-1,-1) -- (0,0) node[at start, below] {local costmap};
            \draw[thick,->] (0.3,-1.8) -- (2,-0.7) node[at start, below] {global costmap};
            \draw[thick,->] (0.3,-1.8) -- (1.9,-1);
            }
        \end{tikzpicture}

    \begin{columns}
        \begin{column}{0.5\linewidth}
            \textbf{Global path planning}

            \only<1>{
            \begin{itemize}
                \item long distances (\ie large map, slower calculations)
                \item static environment
            \end{itemize}
            }
            \only<2> {
                \vspace{1em}
                $\rightarrow$ global (static) \textbf{costmap}
            }
        \end{column}
        \begin{column}{0.5\linewidth}
            \textbf{Local path planning}

            \only<1>{
            \begin{itemize}
                \item Short horizon
                \item Dynamic environment (use of sensors)
            \end{itemize}
            }
            \only<2> {
                \vspace{1em}
                $\rightarrow$ local (dynamic) \textbf{costmap}
            }
        \end{column}
    \end{columns}

\end{frame}

\begin{frame}{Global path planning}

Assumptions

\begin{itemize}
\item There exists a good enough map of the environment for navigation
  (topological or metric or a mixture between both).
\item The robot knows where it is on the map (using for example GPS or Monte
  Carlo localisation).
\end{itemize}

First step:

\begin{itemize}
\item Representation of the environment by a road-map (graph), cells or a
  potential field. The resulting discrete locations or cells allow then
  to use standard planning algorithms.
\end{itemize}

Examples:

\begin{itemize}
\item Visibility Graph
\item Voronoi Diagram
\item Cell Decomposition -\textgreater{} Connectivity Graph
\item Potential Field
\end{itemize}

\end{frame}

\begin{frame}{Path Planning: Configuration Space}

\begin{itemize}
\item State or configuration $q$ can be described with $k$ values
  $qi$

    \begin{center}
        \includegraphics[width=0.6\linewidth]{configurationspace}
    \end{center}

\item \textbf{Configuration space} of a robot: a n-dimensional map of free
  and occupied states.
\end{itemize}

\end{frame}

\begin{frame}{Graphs}

Maps used by robots can be seen as interconnected nodes.

Areas with paths between them are called \textbf{graphs.}

\begin{itemize}
\item \textbf{In maths, the paths are called edges, the areas are called
  nodes.}
\end{itemize}

For example: a visibility graph

    \begin{center}
        \includegraphics[width=0.6\linewidth]{visibilitygraph}
    \end{center}

Edges between nodes are the lines of sight from each corner to an
obstacle to every other visible corner.

\end{frame}

\begin{frame}{Graphs: Voronoi graphs}

\begin{itemize}
\item Voronoi graph, distance to obstacles from every nodes is maximal.

    \begin{center}
        \includegraphics[width=0.8\linewidth]{voronoi}
    \end{center}

\item Works well as the robot's sensors get a get view of the environment.
\end{itemize}

\end{frame}

\begin{frame}{Graphs: Voronoi diagram}

\begin{itemize}
\item SysQuake demo
\end{itemize}

\end{frame}

\begin{frame}{Graphs: Cell Decomposition}

Divide space into simple, connected regions called cells

Determine which open sells are adjacent and construct a connectivity
graph

Find cells in which the initial and goal configuration (state) lie and
search for a path in the connectivity graph to join them.

From the sequence of cells found with an appropriate search algorithm,
compute a path within each cell.

\begin{itemize}
\item e.g. passing through the midpoints of cell boundaries or by sequence
  of wall following movements.
\end{itemize}

\end{frame}

\begin{frame}{Graphs: Cell Decomposition}

Example: exact cell decomposition

    \begin{center}
        \includegraphics[width=0.8\linewidth]{exactcelldecomposition}
    \end{center}

\end{frame}

\begin{frame}{Graphs: Cell Decomposition}

Example: approximate cell decomposition

    \begin{center}
        \includegraphics[width=0.4\linewidth]{celldecomposition1}
        \hspace{1em}
        \includegraphics[width=0.4\linewidth]{celldecomposition2}
    \end{center}

\end{frame}

\begin{frame}{Graphs: Cell Decomposition}

Adaptive cell decomposition: changes the size of each cell according
  to the detail needed.

    \begin{center}
        \includegraphics[width=0.8\linewidth]{adaptivecelldecomposition}
    \end{center}
\end{frame}

\begin{frame}{Graphs: representing}

\begin{itemize}
\item How is a graph represented in a computer? Graphical representation is
  intuitive, but not straightforward to implement in a program.
\item Graphs can be represented as a matrix:
\end{itemize}


    \begin{tikzpicture}[every edge quotes/.style={auto=right}]
        \node at (0,0) (A) {$A$};
        \node at (2,1) (B) {$B$};
        \node at (0,2) (C) {$C$} edge["6m",left] (A)
                                 edge["5m"] (B);
        \node at (1,4) (D) {$D$} edge["10m"] (C);
        \node at (2.5,3.5) (E) {$E$} edge["4m"] (D)
                                     edge["8m"] (B)
                                     edge["25m"] (A);

        %\draw (A) --[label=25m] (E) --[label=4m] (D) --[label=10m] (B) --[label=5m] (C) --[label=6m] (A);
        %\draw (E) --[label=8m] (B);
    \end{tikzpicture}

\end{frame}

\begin{frame}{Searching graphs}

Graphs can be searched, for example to find the shortest path between
two nodes.

Many different search algorithms exist

\begin{itemize}
\item \href{http://en.wikipedia.org/wiki/Breadth-first_search}{Breadth
  first}
\item \href{http://en.wikipedia.org/wiki/Depth-first_search}{Depth first}
\item \href{http://en.wikipedia.org/wiki/A*_search_algorithm}{A*}
\item \href{http://en.wikipedia.org/wiki/Dijkstra's_algorithm}{Dijkstra's
  shortest path}
\end{itemize}

\end{frame}

\begin{frame}[fragile]{Searching graphs - Dijkstra}

\begin{pythoncode}
def dijkstra(graph, weights, start_node):

  dist = {} # map {node -> distance to 'start_node'}
  previous = {} # needed to reconstruct shortest path

  for node in graph:
    dist[node] = math.inf # initial dist. from 'start_node' to 'node'

  dist[start_node] = 0

  Q = graph.nodes() # unvisited nodes
  while not Q.empty():
    u = pop_min(Q) # remove best node

    for v in u.neighbours: # iterate over nodes connected to u
      if dist[u] + weights(u, v) < dist[v]: # new shorter path to v!
        dist[v] = dist[u] + weights(u, v)
        previous[v] = u

  return dist, previous
\end{pythoncode}
\end{frame}


\begin{frame}[fragile]{Searching graphs -- Dijkstra}

Reading out shortest path from the start node to to $t$:

\begin{pythoncode}
path = []
node = t # goal
while node in previous:
    path = [node] + path # append at the front of our path
    node = previous[node]
\end{pythoncode}

Returns the shortest path (if there are more than one, only one is returned).
Return empty if no path exists.

\end{frame}

\begin{frame}{Dijkstra's algorithm: illustration}

    \begin{center}
    \video[1]{0.6\linewidth}{figs/part6/dijkstra.mp4}
    \end{center}

Dijkstra search for finding path between start (red) and goal (green)
position (\href{http://en.wikipedia.org/wiki/Dijkstra's_algorithm}{Wiki
Commons}). Note how Dijkstra is expanding its search out from the
starting position, without knowledge about which nodes could bring it
closer to the goal.

\end{frame}

\begin{frame}{The cost of search}

Maps can be huge

\begin{itemize}
\item For example, 100,000 m2 shopping mall, with a 10x10cm map
  resolution, requires 10 million nodes on the graph.
\end{itemize}

Search time can be problematic

\begin{itemize}
\item Dijkstra has order $O$(N+V2). For example, for a map with 107
  nodes and 108 vertices, it can take 107+102x8 ≈ 1016 calculations to
  find a shortest path between two points.
\item Some implementations are much more efficient. A* search is at worst
  the same as Dijkstra, but can be polynomial in the number of nodes
  $O$(Nk) when a good heuristic is used. For example, if you can
  choose between two nodes to explore, take the node closer to the goal.
\end{itemize}

\end{frame}

\begin{frame}{A* algorithm - illustration}

    \begin{center}
        \video[1]{0.6\linewidth}{figs/part6/astar.mp4}
    \end{center}

A* search for finding path between start and goal position. A* uses a
heuristic (here the distance to the goal
\href{http://en.wikipedia.org/wiki/A*_search_algorithm}{--} it first
expands to nodes closer to the goal) which gives it a chance of finding
the solution in less time.
\href{http://en.wikipedia.org/wiki/A*_search_algorithm}{(Wiki Commons})

\end{frame}

\section{Potential field path planning}

\begin{frame}{Potential Field Path Planning}

Robot is treated as a \emph{point under the influence} of an artificial
potential field.

\begin{itemize}
\item Generated robot movement is similar to a ball rolling down the hill
\item Goal generates attractive force
\item Obstacle are repulsive forces.
\end{itemize}

Needed

\begin{itemize}
\item Location of robot.
\item Location of obstacles.
\item A good way is for these to be on a metric map.
\end{itemize}

\end{frame}

\begin{frame}{Potential Field Path Planning}

    \begin{center}
        \includegraphics[width=0.4\linewidth]{potentialfields1}

        \includegraphics[width=0.4\linewidth]{potentialfields2}
        \includegraphics[width=0.4\linewidth]{potentialfields3}
    \end{center}
\end{frame}

\begin{frame}{Potential Field Path Planning}

Potential field generation

Generation of potential field function $U(q)$

\begin{itemize}
\item attracting (goal) and repulsing (obstacle) fields
\item summing up the fields
\item functions must be
  \href{http://en.wikipedia.org/wiki/Differentiable_function}{differentiable}
\end{itemize}

Generate artificial force field $F(q)$

\[
    F(q) = -\nabla U(q) = -\nabla U_{att}(q) - \nabla U_{rep}(q) =
    \begin{bmatrix}\frac{\partial U}{x} \\ \frac{\partial U}{y}\end{bmatrix}
\]

Set robot speed $(v_x, v_y)$ proportional to the force
$F(q)$ generated by the field

\begin{itemize}
\item the force field drives the robot to the goal
\item if robot is assumed to be a point mass
\end{itemize}


\end{frame}

\begin{frame}{Potential Field Path Planning}

Attractive potential field

\begin{itemize}
\item For example, a parabolic function representing the Euclidean distance
  to the goal
\item Attracting force converges linearly towards 0 (goal)
\end{itemize}

\end{frame}

\begin{frame}{Potential Field Path Planning}

Repulsing potential field

Should generate a barrier around all the obstacle

\begin{itemize}
\item strong if close to the obstacle
\item no influence if far from the obstacle
\item Field is positive or zero and \emph{tends to infinity} as q gets
  closer to the object
\end{itemize}

\end{frame}

\begin{frame}{Potential Field Path Planning}

Notes:

\begin{itemize}
\item Local minima problem exists: robot can get stuck in places on the map
  where the potential field has a local dip.
\item problem is getting more complex if the robot is not considered as a
  point mass
\item If objects are convex there exists situations where several minimal
  distances exist ® can result in oscillations
\item Robot path oscillates, e.g. in narrow passages.
\end{itemize}

Sysquake demo

Videos

\begin{itemize}
\item \url{http://www.youtube.com/watch?v=DxzRYYMjxKY}
\item \url{http://www.youtube.com/watch?v=JnPfu7xHNt4}
\end{itemize}

\end{frame}

\begin{frame}{Potential Field Path Planning}

Extended Potential Field Path Planning

Additionally a \emph{rotation potential field} and a \emph{task
potential field} in introduced

Rotation potential field

\begin{itemize}
\item force is also a function of robots orientation to the obstacle
\end{itemize}

Task potential field

\begin{itemize}
\item Filters out the obstacles that should not influence the robots
  movements, i.e. only the obstacles in the sector Z in front of the
  robot are considered
\end{itemize}

\end{frame}

\section{Grassfire algorithm}

\begin{frame}{Grassfire algorithm}

\begin{itemize}
\item Starting from a metric map, a wave front is expanded around the goal.
\item Goal is reaches by travelling on descending values.
\end{itemize}

    \begin{center}
        \includegraphics[width=0.6\linewidth]{grassfire}
    \end{center}
\end{frame}

\begin{frame}{Grassfire algorithm}

\end{frame}

\imageframe{grassfire2}

\begin{frame}[fragile]{Grassfire algorithm}

\begin{pythoncode}

def grassfire( M, goal )
  Q = goal # Put goal in queue
  M(goal) := 0 # Set goal value to 0

  while Q is not empty: # Do until map filled
    a := dequeue from Q
    for n := all neighbours of a AND if neighbour no value yet
      if n is not obstacle
        Q := Q enqueue n
        M(n) = M(a) + 1
\end{pythoncode}

Q is a
\href{http://en.wikipedia.org/wiki/Queue_(abstract_data_type)}{queue
data structure}, a first-in first-out list. The queue keeps track of
which locations on the map still need to be visited.

\end{frame}


\begin{frame}{}
    \begin{center}
        \Large
        That's all, folks!\\[2em]
        \normalsize
        Questions:\\
        Portland Square A216 or \url{severin.lemaignan@plymouth.ac.uk} \\[1em]

        Slides:\\ \href{https://github.com/severin-lemaignan/module-mobile-and-humanoid-robots}{\small github.com/severin-lemaignan/module-mobile-and-humanoid-robots}

    \end{center}
\end{frame}



\end{document}
